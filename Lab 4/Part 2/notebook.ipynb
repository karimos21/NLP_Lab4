{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Part 1: Loading Data and Tokenization"]},{"cell_type":"markdown","metadata":{},"source":["This part focuses on getting the fairy tale text ready for training. It includes loading the data, tokenizing it using GPT-2's tokenizer, and splitting it into manageable sequences."]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T14:29:13.277922Z","iopub.status.busy":"2024-05-26T14:29:13.277004Z","iopub.status.idle":"2024-05-26T14:38:02.721488Z","shell.execute_reply":"2024-05-26T14:38:02.720554Z","shell.execute_reply.started":"2024-05-26T14:29:13.277878Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"591c203159754db6809fc467a9cab53b","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6c1cf59ef64a4814a32a7a3f7175df15","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c91f5687eef45ea9492f1c859d5dad0","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"07a30f6d56214c66bf4e548b4354e3a4","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee5feb67e7f84a048624bbc95a2e82af","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["The Happy Prince.\n","HIGH above the city, on a tall column, stood the statue of the Happy Prince.  He was gilded all over with thin leaves of fine gold, for eyes he had two bright sapphires, and a large red ruby glowed on his sword-hilt.\n","He was very much admired indeed.  “He is as beautiful as a weathercock,” remarked one of the Town Councillors who wished to gain a reputation for having artistic tastes; “only not quite so useful,” he added, fearing lest people should think him unpractical, which h\n"]},{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (5104911 > 1024). Running this sequence through the model will result in indexing errors\n"]},{"name":"stdout","output_type":"stream","text":["Input 1: The Happy Prince.\n","HIGH above the city, on a tall column, stood the statue of the Happy Prince.  He was gilded all over with thin leaves of fine gold, for eyes he had two bright sapphires, and a large ...\n","Input 2:  really was not.\n","“Why can’t you be like the Happy Prince?” asked a sensible mother of her little boy who was crying for the moon.  “The Happy Prince never dreams of crying for anything.”\n","“I am glad th...\n","Input 3:  know?” said the Mathematical Master, “you have never seen one.”\n","“Ah! but we have, in our dreams,” answered the children; and the Mathematical Master frowned and looked very severe, for he did not app...\n","Input 4:  by her slender waist that he had stopped to talk to her.\n","“Shall I love you?” said the Swallow, who liked to come to the point at once, and the Reed made him a low bow.  So he flew round and round her...\n","Input 5: .  Then, when the autumn came they all flew away.\n","After they had gone he felt lonely, and began to tire of his lady-love.\n","“She has no conversation,” he said, “and I am afraid that she is a coquette, f...\n"]}],"source":["from transformers import GPT2Tokenizer\n","\n","# Load the GPT-2 tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","# Define the path to the text file\n","file_path = \"/kaggle/input/children-stories-text-corpus/cleaned_merged_fairy_tales_without_eos.txt\"\n","\n","# Read the content of the text file\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    story_text = file.read()\n","\n","# Print the first few characters of the text\n","print(story_text[:500])\n","\n","# Tokenize the text\n","tokenized_text = tokenizer.encode(story_text, return_tensors=\"pt\")\n","\n","# Split the tokenized text into smaller sequences\n","max_length = 512  # Maximum sequence length for GPT-2\n","stride = 128  # Stride for splitting the text\n","input_sequences = []\n","\n","for i in range(0, tokenized_text.size(1), stride):\n","    input_sequences.append(tokenized_text[0, i : i + max_length])\n","\n","# Convert input sequences to list of strings\n","input_texts = [tokenizer.decode(seq, skip_special_tokens=True) for seq in input_sequences]\n","\n","# Print the first few input sequences\n","for i, input_text in enumerate(input_texts[:5]):\n","    print(f\"Input {i + 1}: {input_text[:200]}...\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Part 2: Chunking Text for Model Input"]},{"cell_type":"markdown","metadata":{},"source":["This section breaks down the text into smaller chunks that fit within the maximum sequence length of the GPT-2 model. This is essential for preventing memory issues during training."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T15:10:41.213874Z","iopub.status.busy":"2024-05-26T15:10:41.213519Z","iopub.status.idle":"2024-05-26T15:11:30.225557Z","shell.execute_reply":"2024-05-26T15:11:30.224537Z","shell.execute_reply.started":"2024-05-26T15:10:41.213848Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of input sequences: 19977\n"]}],"source":["from transformers import GPT2Tokenizer\n","\n","# Load the GPT-2 tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","# Define the maximum sequence length for the model\n","max_length = 1024\n","\n","# Split the text into smaller chunks\n","chunk_size = 1024\n","chunks = [story_text[i:i+chunk_size] for i in range(0, len(story_text), chunk_size)]\n","\n","# Tokenize and preprocess each chunk\n","input_sequences = []\n","for i, chunk in enumerate(chunks):\n","    # Tokenize the chunk\n","    tokenized_chunk = tokenizer.encode(chunk, return_tensors=\"pt\")\n","\n","    # Ensure the chunk fits within the maximum sequence length\n","    if tokenized_chunk.size(1) > max_length:\n","        tokenized_chunk = tokenized_chunk[:, :max_length]\n","\n","    # Append the tokenized chunk to the input sequences\n","    input_sequences.append(tokenized_chunk)\n","\n","# Print the number of input sequences\n","print(\"Number of input sequences:\", len(input_sequences))\n"]},{"cell_type":"markdown","metadata":{},"source":["# Part 3: Fine-Tuning GPT-2 on the Fairy Tale Dataset"]},{"cell_type":"markdown","metadata":{},"source":["This is the core training process. It loads the pre-trained GPT-2 model, defines the training dataset and data collator, sets up the training arguments, and then starts the fine-tuning process."]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T15:13:37.962127Z","iopub.status.busy":"2024-05-26T15:13:37.961261Z","iopub.status.idle":"2024-05-26T15:56:06.532699Z","shell.execute_reply":"2024-05-26T15:56:06.531468Z","shell.execute_reply.started":"2024-05-26T15:13:37.962093Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.17.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.16.6"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20240526_151459-kth88hjh</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/dkfji/huggingface/runs/kth88hjh' target=\"_blank\">peachy-leaf-2</a></strong> to <a href='https://wandb.ai/dkfji/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/dkfji/huggingface' target=\"_blank\">https://wandb.ai/dkfji/huggingface</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/dkfji/huggingface/runs/kth88hjh' target=\"_blank\">https://wandb.ai/dkfji/huggingface/runs/kth88hjh</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='14958' max='14958' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [14958/14958 40:48, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>3.630000</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>3.548200</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>3.499900</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>3.470600</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>3.447900</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>3.429700</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>3.419100</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>3.419000</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>3.402800</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>3.367000</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>3.279800</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>3.286500</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>3.268400</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>3.258500</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>3.267500</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>3.266800</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>3.265600</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>3.244200</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>3.254200</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>3.242400</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>3.174900</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>3.181900</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>3.177200</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>3.185600</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>3.170900</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>3.180600</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>3.176700</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>3.170900</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>3.171100</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=14958, training_loss=3.3015304914306935, metrics={'train_runtime': 2504.7272, 'train_samples_per_second': 47.768, 'train_steps_per_second': 5.972, 'total_flos': 7815636615168000.0, 'train_loss': 3.3015304914306935, 'epoch': 3.0})"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments\n","import os\n","\n","# Load the pre-trained GPT-2 model and tokenizer\n","model_name = \"gpt2\"\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","\n","# Define the file path and cache directory\n","file_path = \"/kaggle/input/children-stories-text-corpus/cleaned_merged_fairy_tales_without_eos.txt\"\n","cache_dir = \"/kaggle/working/cache\"  # Writable directory for caching\n","\n","# Ensure the cache directory exists\n","os.makedirs(cache_dir, exist_ok=True)\n","\n","# Define the training dataset\n","train_dataset = TextDataset(\n","    tokenizer=tokenizer,\n","    file_path=file_path,\n","    block_size=128,\n","    overwrite_cache=True,\n","    cache_dir=cache_dir,  # Use the cache directory\n",")\n","\n","# Define the data collator\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, \n","    mlm=False\n",")\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./gpt2_finetuned\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=3,\n","    per_device_train_batch_size=8,\n","    save_steps=10_000,\n","    save_total_limit=2,\n","    prediction_loss_only=True,\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n",")\n","\n","# Fine-tune the model\n","trainer.train()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T15:58:06.530279Z","iopub.status.busy":"2024-05-26T15:58:06.529527Z","iopub.status.idle":"2024-05-26T15:58:07.394898Z","shell.execute_reply":"2024-05-26T15:58:07.393861Z","shell.execute_reply.started":"2024-05-26T15:58:06.530247Z"},"trusted":true},"outputs":[{"data":{"text/plain":["('./fine_tuned_model/tokenizer_config.json',\n"," './fine_tuned_model/special_tokens_map.json',\n"," './fine_tuned_model/vocab.json',\n"," './fine_tuned_model/merges.txt',\n"," './fine_tuned_model/added_tokens.json')"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["output_dir = './fine_tuned_model'\n","trainer.save_model(output_dir)\n","tokenizer.save_pretrained(output_dir)"]},{"cell_type":"markdown","metadata":{},"source":["# Part 4: Generating Text with the Fine-Tuned Model"]},{"cell_type":"markdown","metadata":{},"source":["This part demonstrates how to use the fine-tuned model to generate new fairy tale text. It includes encoding an input prompt, using the model's generate function, and decoding the output."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T15:59:04.570203Z","iopub.status.busy":"2024-05-26T15:59:04.569315Z","iopub.status.idle":"2024-05-26T15:59:08.136107Z","shell.execute_reply":"2024-05-26T15:59:08.135096Z","shell.execute_reply.started":"2024-05-26T15:59:04.570169Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Once upon a time, when the sun was shining brightly, the little girl was sitting on the grass, and the little man was sitting on the tree.\n","\"What is the matter?\" asked the little man.\n","\"I am going to the forest to hunt,\" said the little girl.\n","\"What is the matter?\" asked the man.\n","\"I am going to the forest to hunt,\" said the little girl.\n","\"What is the matter?\" asked the man.\n","\"I\n"]}],"source":["from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","# Load the fine-tuned model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained(output_dir)\n","tokenizer = GPT2Tokenizer.from_pretrained(output_dir)\n","\n","# Encode input text\n","input_text = \"Once upon a time\"\n","input_ids = tokenizer.encode(input_text, return_tensors='pt')\n","\n","# Generate text\n","output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n","\n","# Decode generated text\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(generated_text)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T15:59:44.300654Z","iopub.status.busy":"2024-05-26T15:59:44.299880Z","iopub.status.idle":"2024-05-26T15:59:47.916313Z","shell.execute_reply":"2024-05-26T15:59:47.914413Z","shell.execute_reply.started":"2024-05-26T15:59:44.300622Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Once upon a time, when the sun was shining brightly, the little girl was sitting on the grass, and the old woman was standing by the fire, looking at her with a sad face.\n","\"What is the matter?\" she asked. \"What are you doing here?\"\n","The old man answered, \"I am going to the forest to hunt for my lost brother.\"\n","Then the girl said, \n","  \"My brother is dead, but I am still alive, for I have\n"]}],"source":["input_text = \"Once upon a time\"\n","output = model.generate(input_ids, max_length=100, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n","\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(generated_text)"]},{"cell_type":"markdown","metadata":{},"source":["# Part 5: Adjusting Generation Parameters"]},{"cell_type":"markdown","metadata":{},"source":["This section shows how to control the creativity and style of the generated text by adjusting parameters like temperature, top_k, and top_p."]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T16:02:42.384992Z","iopub.status.busy":"2024-05-26T16:02:42.384481Z","iopub.status.idle":"2024-05-26T16:02:45.601507Z","shell.execute_reply":"2024-05-26T16:02:45.600402Z","shell.execute_reply.started":"2024-05-26T16:02:42.384958Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Once upon a time, when the sun was shining brightly, the little girl was sitting on the grass, and the old woman was standing by the fire, looking at her with a sad face.\n","\"What is the matter?\" she asked. \"What are you doing here?\"\n","The old man answered, \"I am going to the forest to hunt for my lost brother.\"\n","Then the girl said, \n","  \"My brother is dead, but I am still alive, for I have\n"]}],"source":["output = model.generate(\n","    input_ids,\n","    max_length=100,\n","    num_return_sequences=1,\n","    no_repeat_ngram_size=2,\n","    early_stopping=True,\n","    temperature=0.7,  # Adjust temperature to control creativity\n","    top_k=50,  # Adjust top_k for top-k sampling\n","    top_p=0.95  # Adjust top_p for nucleus sampling\n",")\n","\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(generated_text)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Part 6: Further Fine-Tuning "]},{"cell_type":"markdown","metadata":{},"source":["This part explores further fine-tuning options, like using a different learning rate or increasing the number of training epochs, to enhance the model's performance."]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T16:03:21.377522Z","iopub.status.busy":"2024-05-26T16:03:21.376777Z","iopub.status.idle":"2024-05-26T18:00:15.864610Z","shell.execute_reply":"2024-05-26T18:00:15.863626Z","shell.execute_reply.started":"2024-05-26T16:03:21.377494Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='99705' max='99705' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [99705/99705 1:56:53, Epoch 5/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>3.199500</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>3.242000</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>3.266200</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>3.231900</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>3.236500</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>3.251900</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>3.282200</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>3.267800</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>3.261000</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>3.253300</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>3.252500</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>3.262700</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>3.286100</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>3.240000</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>3.265500</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>3.244600</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>3.248000</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>3.255000</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>3.259600</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>3.267000</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>3.242200</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>3.250700</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>3.267800</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>3.256800</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>3.243600</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>3.264700</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>3.248000</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>3.270600</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>3.275800</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>3.264200</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>3.259600</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>3.269700</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>3.242200</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>3.248400</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>3.279700</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>3.255300</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>3.234100</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>3.231200</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>3.226200</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>3.227800</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>3.058100</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>3.067700</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>3.053200</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>3.063600</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>3.050500</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>3.081800</td>\n","    </tr>\n","    <tr>\n","      <td>23500</td>\n","      <td>3.083000</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>3.103700</td>\n","    </tr>\n","    <tr>\n","      <td>24500</td>\n","      <td>3.061500</td>\n","    </tr>\n","    <tr>\n","      <td>25000</td>\n","      <td>3.081600</td>\n","    </tr>\n","    <tr>\n","      <td>25500</td>\n","      <td>3.075200</td>\n","    </tr>\n","    <tr>\n","      <td>26000</td>\n","      <td>3.069400</td>\n","    </tr>\n","    <tr>\n","      <td>26500</td>\n","      <td>3.068000</td>\n","    </tr>\n","    <tr>\n","      <td>27000</td>\n","      <td>3.083800</td>\n","    </tr>\n","    <tr>\n","      <td>27500</td>\n","      <td>3.056500</td>\n","    </tr>\n","    <tr>\n","      <td>28000</td>\n","      <td>3.078600</td>\n","    </tr>\n","    <tr>\n","      <td>28500</td>\n","      <td>3.085400</td>\n","    </tr>\n","    <tr>\n","      <td>29000</td>\n","      <td>3.096100</td>\n","    </tr>\n","    <tr>\n","      <td>29500</td>\n","      <td>3.069800</td>\n","    </tr>\n","    <tr>\n","      <td>30000</td>\n","      <td>3.090800</td>\n","    </tr>\n","    <tr>\n","      <td>30500</td>\n","      <td>3.072600</td>\n","    </tr>\n","    <tr>\n","      <td>31000</td>\n","      <td>3.095700</td>\n","    </tr>\n","    <tr>\n","      <td>31500</td>\n","      <td>3.102100</td>\n","    </tr>\n","    <tr>\n","      <td>32000</td>\n","      <td>3.094700</td>\n","    </tr>\n","    <tr>\n","      <td>32500</td>\n","      <td>3.101400</td>\n","    </tr>\n","    <tr>\n","      <td>33000</td>\n","      <td>3.082800</td>\n","    </tr>\n","    <tr>\n","      <td>33500</td>\n","      <td>3.105400</td>\n","    </tr>\n","    <tr>\n","      <td>34000</td>\n","      <td>3.106600</td>\n","    </tr>\n","    <tr>\n","      <td>34500</td>\n","      <td>3.091200</td>\n","    </tr>\n","    <tr>\n","      <td>35000</td>\n","      <td>3.069300</td>\n","    </tr>\n","    <tr>\n","      <td>35500</td>\n","      <td>3.087800</td>\n","    </tr>\n","    <tr>\n","      <td>36000</td>\n","      <td>3.086700</td>\n","    </tr>\n","    <tr>\n","      <td>36500</td>\n","      <td>3.099800</td>\n","    </tr>\n","    <tr>\n","      <td>37000</td>\n","      <td>3.096400</td>\n","    </tr>\n","    <tr>\n","      <td>37500</td>\n","      <td>3.079200</td>\n","    </tr>\n","    <tr>\n","      <td>38000</td>\n","      <td>3.104000</td>\n","    </tr>\n","    <tr>\n","      <td>38500</td>\n","      <td>3.077100</td>\n","    </tr>\n","    <tr>\n","      <td>39000</td>\n","      <td>3.102800</td>\n","    </tr>\n","    <tr>\n","      <td>39500</td>\n","      <td>3.098300</td>\n","    </tr>\n","    <tr>\n","      <td>40000</td>\n","      <td>3.045500</td>\n","    </tr>\n","    <tr>\n","      <td>40500</td>\n","      <td>2.911000</td>\n","    </tr>\n","    <tr>\n","      <td>41000</td>\n","      <td>2.930100</td>\n","    </tr>\n","    <tr>\n","      <td>41500</td>\n","      <td>2.916900</td>\n","    </tr>\n","    <tr>\n","      <td>42000</td>\n","      <td>2.923000</td>\n","    </tr>\n","    <tr>\n","      <td>42500</td>\n","      <td>2.938000</td>\n","    </tr>\n","    <tr>\n","      <td>43000</td>\n","      <td>2.913800</td>\n","    </tr>\n","    <tr>\n","      <td>43500</td>\n","      <td>2.959500</td>\n","    </tr>\n","    <tr>\n","      <td>44000</td>\n","      <td>2.923300</td>\n","    </tr>\n","    <tr>\n","      <td>44500</td>\n","      <td>2.958900</td>\n","    </tr>\n","    <tr>\n","      <td>45000</td>\n","      <td>2.923700</td>\n","    </tr>\n","    <tr>\n","      <td>45500</td>\n","      <td>2.958700</td>\n","    </tr>\n","    <tr>\n","      <td>46000</td>\n","      <td>2.920100</td>\n","    </tr>\n","    <tr>\n","      <td>46500</td>\n","      <td>2.957400</td>\n","    </tr>\n","    <tr>\n","      <td>47000</td>\n","      <td>2.957600</td>\n","    </tr>\n","    <tr>\n","      <td>47500</td>\n","      <td>2.955900</td>\n","    </tr>\n","    <tr>\n","      <td>48000</td>\n","      <td>2.958100</td>\n","    </tr>\n","    <tr>\n","      <td>48500</td>\n","      <td>2.969900</td>\n","    </tr>\n","    <tr>\n","      <td>49000</td>\n","      <td>2.932800</td>\n","    </tr>\n","    <tr>\n","      <td>49500</td>\n","      <td>2.959700</td>\n","    </tr>\n","    <tr>\n","      <td>50000</td>\n","      <td>2.934700</td>\n","    </tr>\n","    <tr>\n","      <td>50500</td>\n","      <td>2.967800</td>\n","    </tr>\n","    <tr>\n","      <td>51000</td>\n","      <td>2.963200</td>\n","    </tr>\n","    <tr>\n","      <td>51500</td>\n","      <td>2.969800</td>\n","    </tr>\n","    <tr>\n","      <td>52000</td>\n","      <td>2.952100</td>\n","    </tr>\n","    <tr>\n","      <td>52500</td>\n","      <td>2.972500</td>\n","    </tr>\n","    <tr>\n","      <td>53000</td>\n","      <td>2.962200</td>\n","    </tr>\n","    <tr>\n","      <td>53500</td>\n","      <td>2.984300</td>\n","    </tr>\n","    <tr>\n","      <td>54000</td>\n","      <td>2.946500</td>\n","    </tr>\n","    <tr>\n","      <td>54500</td>\n","      <td>2.957500</td>\n","    </tr>\n","    <tr>\n","      <td>55000</td>\n","      <td>2.950800</td>\n","    </tr>\n","    <tr>\n","      <td>55500</td>\n","      <td>2.998800</td>\n","    </tr>\n","    <tr>\n","      <td>56000</td>\n","      <td>2.949700</td>\n","    </tr>\n","    <tr>\n","      <td>56500</td>\n","      <td>2.969900</td>\n","    </tr>\n","    <tr>\n","      <td>57000</td>\n","      <td>2.950500</td>\n","    </tr>\n","    <tr>\n","      <td>57500</td>\n","      <td>2.972800</td>\n","    </tr>\n","    <tr>\n","      <td>58000</td>\n","      <td>2.971300</td>\n","    </tr>\n","    <tr>\n","      <td>58500</td>\n","      <td>2.960100</td>\n","    </tr>\n","    <tr>\n","      <td>59000</td>\n","      <td>2.960300</td>\n","    </tr>\n","    <tr>\n","      <td>59500</td>\n","      <td>2.993400</td>\n","    </tr>\n","    <tr>\n","      <td>60000</td>\n","      <td>2.940900</td>\n","    </tr>\n","    <tr>\n","      <td>60500</td>\n","      <td>2.833100</td>\n","    </tr>\n","    <tr>\n","      <td>61000</td>\n","      <td>2.824500</td>\n","    </tr>\n","    <tr>\n","      <td>61500</td>\n","      <td>2.821200</td>\n","    </tr>\n","    <tr>\n","      <td>62000</td>\n","      <td>2.854400</td>\n","    </tr>\n","    <tr>\n","      <td>62500</td>\n","      <td>2.843200</td>\n","    </tr>\n","    <tr>\n","      <td>63000</td>\n","      <td>2.837700</td>\n","    </tr>\n","    <tr>\n","      <td>63500</td>\n","      <td>2.847600</td>\n","    </tr>\n","    <tr>\n","      <td>64000</td>\n","      <td>2.850000</td>\n","    </tr>\n","    <tr>\n","      <td>64500</td>\n","      <td>2.836000</td>\n","    </tr>\n","    <tr>\n","      <td>65000</td>\n","      <td>2.825300</td>\n","    </tr>\n","    <tr>\n","      <td>65500</td>\n","      <td>2.839400</td>\n","    </tr>\n","    <tr>\n","      <td>66000</td>\n","      <td>2.861800</td>\n","    </tr>\n","    <tr>\n","      <td>66500</td>\n","      <td>2.838600</td>\n","    </tr>\n","    <tr>\n","      <td>67000</td>\n","      <td>2.860700</td>\n","    </tr>\n","    <tr>\n","      <td>67500</td>\n","      <td>2.871100</td>\n","    </tr>\n","    <tr>\n","      <td>68000</td>\n","      <td>2.875600</td>\n","    </tr>\n","    <tr>\n","      <td>68500</td>\n","      <td>2.851800</td>\n","    </tr>\n","    <tr>\n","      <td>69000</td>\n","      <td>2.863000</td>\n","    </tr>\n","    <tr>\n","      <td>69500</td>\n","      <td>2.839600</td>\n","    </tr>\n","    <tr>\n","      <td>70000</td>\n","      <td>2.864100</td>\n","    </tr>\n","    <tr>\n","      <td>70500</td>\n","      <td>2.839400</td>\n","    </tr>\n","    <tr>\n","      <td>71000</td>\n","      <td>2.869500</td>\n","    </tr>\n","    <tr>\n","      <td>71500</td>\n","      <td>2.855600</td>\n","    </tr>\n","    <tr>\n","      <td>72000</td>\n","      <td>2.869900</td>\n","    </tr>\n","    <tr>\n","      <td>72500</td>\n","      <td>2.828200</td>\n","    </tr>\n","    <tr>\n","      <td>73000</td>\n","      <td>2.861900</td>\n","    </tr>\n","    <tr>\n","      <td>73500</td>\n","      <td>2.849600</td>\n","    </tr>\n","    <tr>\n","      <td>74000</td>\n","      <td>2.855700</td>\n","    </tr>\n","    <tr>\n","      <td>74500</td>\n","      <td>2.854900</td>\n","    </tr>\n","    <tr>\n","      <td>75000</td>\n","      <td>2.844100</td>\n","    </tr>\n","    <tr>\n","      <td>75500</td>\n","      <td>2.853000</td>\n","    </tr>\n","    <tr>\n","      <td>76000</td>\n","      <td>2.841300</td>\n","    </tr>\n","    <tr>\n","      <td>76500</td>\n","      <td>2.879000</td>\n","    </tr>\n","    <tr>\n","      <td>77000</td>\n","      <td>2.829300</td>\n","    </tr>\n","    <tr>\n","      <td>77500</td>\n","      <td>2.841100</td>\n","    </tr>\n","    <tr>\n","      <td>78000</td>\n","      <td>2.855100</td>\n","    </tr>\n","    <tr>\n","      <td>78500</td>\n","      <td>2.875300</td>\n","    </tr>\n","    <tr>\n","      <td>79000</td>\n","      <td>2.891800</td>\n","    </tr>\n","    <tr>\n","      <td>79500</td>\n","      <td>2.865300</td>\n","    </tr>\n","    <tr>\n","      <td>80000</td>\n","      <td>2.823400</td>\n","    </tr>\n","    <tr>\n","      <td>80500</td>\n","      <td>2.765900</td>\n","    </tr>\n","    <tr>\n","      <td>81000</td>\n","      <td>2.739700</td>\n","    </tr>\n","    <tr>\n","      <td>81500</td>\n","      <td>2.767700</td>\n","    </tr>\n","    <tr>\n","      <td>82000</td>\n","      <td>2.793300</td>\n","    </tr>\n","    <tr>\n","      <td>82500</td>\n","      <td>2.760600</td>\n","    </tr>\n","    <tr>\n","      <td>83000</td>\n","      <td>2.783100</td>\n","    </tr>\n","    <tr>\n","      <td>83500</td>\n","      <td>2.777400</td>\n","    </tr>\n","    <tr>\n","      <td>84000</td>\n","      <td>2.786800</td>\n","    </tr>\n","    <tr>\n","      <td>84500</td>\n","      <td>2.783800</td>\n","    </tr>\n","    <tr>\n","      <td>85000</td>\n","      <td>2.810000</td>\n","    </tr>\n","    <tr>\n","      <td>85500</td>\n","      <td>2.770600</td>\n","    </tr>\n","    <tr>\n","      <td>86000</td>\n","      <td>2.769700</td>\n","    </tr>\n","    <tr>\n","      <td>86500</td>\n","      <td>2.797300</td>\n","    </tr>\n","    <tr>\n","      <td>87000</td>\n","      <td>2.762000</td>\n","    </tr>\n","    <tr>\n","      <td>87500</td>\n","      <td>2.767700</td>\n","    </tr>\n","    <tr>\n","      <td>88000</td>\n","      <td>2.772000</td>\n","    </tr>\n","    <tr>\n","      <td>88500</td>\n","      <td>2.795600</td>\n","    </tr>\n","    <tr>\n","      <td>89000</td>\n","      <td>2.753300</td>\n","    </tr>\n","    <tr>\n","      <td>89500</td>\n","      <td>2.759700</td>\n","    </tr>\n","    <tr>\n","      <td>90000</td>\n","      <td>2.780100</td>\n","    </tr>\n","    <tr>\n","      <td>90500</td>\n","      <td>2.755100</td>\n","    </tr>\n","    <tr>\n","      <td>91000</td>\n","      <td>2.781700</td>\n","    </tr>\n","    <tr>\n","      <td>91500</td>\n","      <td>2.779000</td>\n","    </tr>\n","    <tr>\n","      <td>92000</td>\n","      <td>2.780200</td>\n","    </tr>\n","    <tr>\n","      <td>92500</td>\n","      <td>2.780700</td>\n","    </tr>\n","    <tr>\n","      <td>93000</td>\n","      <td>2.794900</td>\n","    </tr>\n","    <tr>\n","      <td>93500</td>\n","      <td>2.783900</td>\n","    </tr>\n","    <tr>\n","      <td>94000</td>\n","      <td>2.739000</td>\n","    </tr>\n","    <tr>\n","      <td>94500</td>\n","      <td>2.770600</td>\n","    </tr>\n","    <tr>\n","      <td>95000</td>\n","      <td>2.761000</td>\n","    </tr>\n","    <tr>\n","      <td>95500</td>\n","      <td>2.787300</td>\n","    </tr>\n","    <tr>\n","      <td>96000</td>\n","      <td>2.751200</td>\n","    </tr>\n","    <tr>\n","      <td>96500</td>\n","      <td>2.780600</td>\n","    </tr>\n","    <tr>\n","      <td>97000</td>\n","      <td>2.761600</td>\n","    </tr>\n","    <tr>\n","      <td>97500</td>\n","      <td>2.766400</td>\n","    </tr>\n","    <tr>\n","      <td>98000</td>\n","      <td>2.785400</td>\n","    </tr>\n","    <tr>\n","      <td>98500</td>\n","      <td>2.804200</td>\n","    </tr>\n","    <tr>\n","      <td>99000</td>\n","      <td>2.785400</td>\n","    </tr>\n","    <tr>\n","      <td>99500</td>\n","      <td>2.759400</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=99705, training_loss=2.9831418253917414, metrics={'train_runtime': 7013.85, 'train_samples_per_second': 28.431, 'train_steps_per_second': 14.215, 'total_flos': 1.302606102528e+16, 'train_loss': 2.9831418253917414, 'epoch': 5.0})"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Fine-tuning with a different learning rate or more epochs\n","training_args = TrainingArguments(\n","    output_dir='./fine_tuned_model',\n","    overwrite_output_dir=True,\n","    num_train_epochs=5,  # adjusting the number of epochs\n","    per_device_train_batch_size=2,\n","    save_steps=10_000,\n","    save_total_limit=2,\n","    prediction_loss_only=True,\n","    learning_rate=5e-5,  # Experiment with the learning rate\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset,\n",")\n","\n","trainer.train()"]},{"cell_type":"markdown","metadata":{},"source":["# Part 7: Generating Text with Adjusted Parameters\n"]},{"cell_type":"markdown","metadata":{},"source":[" This final section focuses on using the fine-tuned model with adjusted parameters to generate more creative and engaging fairy tale stories."]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-05-26T18:13:36.889232Z","iopub.status.busy":"2024-05-26T18:13:36.888692Z","iopub.status.idle":"2024-05-26T18:13:41.955413Z","shell.execute_reply":"2024-05-26T18:13:41.953922Z","shell.execute_reply.started":"2024-05-26T18:13:36.889193Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:497: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:535: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n","  warnings.warn(\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"name":"stdout","output_type":"stream","text":["Once upon a time, when the sun was shining brightly, the little girl was sitting on the grass, and the old woman was standing by the fire, looking at her with a sad face.\n","\"What is the matter?\" she asked. \"What are you doing here?\"\n","The old man answered, \"I am going to the forest to hunt for my lost brother.\"\n","Then the girl said, \n","  \"My brother is dead, but I am still alive, for I have been hunting for him for many years.  I will go and look for the lost man, who is lying in the wood. I want to see if he is alive or dead.\"  Then she went to him and said:\n","\"'I will\n"]}],"source":["from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Loading the fine-tuned model and tokenizer\n","model_name = \"./fine_tuned_model\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","model = GPT2LMHeadModel.from_pretrained(model_name)\n","\n","# Input prompt\n","prompt = \"Once upon a time, when the sun was shining brightly,\"\n","\n","# Encode the input prompt\n","input_ids = tokenizer.encode(prompt, return_tensors='pt')\n","\n","# Generating text with adjusted parameters\n","output = model.generate(\n","    input_ids,\n","    max_length=150,  # we can increase max_length for longer text generation\n","    num_return_sequences=1,\n","    no_repeat_ngram_size=2,\n","    early_stopping=True,\n","    temperature=0.8,  # Adjusting temperature to control creativity\n","    top_k=50,  # Adjusting top_k for top-k sampling\n","    top_p=0.9  # Adjusting top_p for nucleus sampling\n",")\n","\n","# Decode and print the generated text\n","generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n","print(generated_text)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":1469726,"sourceId":2428742,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
