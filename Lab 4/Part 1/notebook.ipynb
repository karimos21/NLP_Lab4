{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Scraping and storing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# List of website URLs\n",
    "urls = [\n",
    "    \"https://www.aljazeera.net/encyclopedia/2016/6/6/%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1-%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A-%D8%AD%D9%8A%D9%86%D9%85%D8%A7-%D8%AA%D9%81%D9%83%D8%B1-%D8%A7%D9%84%D8%A2%D9%84%D8%A9\",\n",
    "    \"https://www.oracle.com/ae-ar/artificial-intelligence/what-is-ai/\",\n",
    "    \"https://aws.amazon.com/ar/what-is/artificial-intelligence/\"\n",
    "]\n",
    "\n",
    "# Function to scrape paragraphs from a URL\n",
    "def scrape_paragraphs_from_url(url):\n",
    "    try:\n",
    "        # Send HTTP request\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for invalid responses\n",
    "\n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract paragraphs\n",
    "        paragraphs = [p.get_text().strip() for p in soup.find_all(\"p\") if p.get_text().strip()]\n",
    "\n",
    "        return paragraphs\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main function to scrape paragraphs from all URLs\n",
    "def scrape_paragraphs_from_all_urls(urls):\n",
    "    all_paragraphs = []\n",
    "    for url in urls:\n",
    "        paragraphs = scrape_paragraphs_from_url(url)\n",
    "        if paragraphs:\n",
    "            all_paragraphs.extend(paragraphs)\n",
    "    return all_paragraphs\n",
    "\n",
    "# Calling the main function to scrape paragraphs from all URLs\n",
    "all_paragraphs = scrape_paragraphs_from_all_urls(urls)\n",
    "\n",
    "# Write paragraphs to a CSV file (without scores)\n",
    "with open(\"paragraphs.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    for paragraph in all_paragraphs:\n",
    "        if paragraph:\n",
    "            writer.writerow([paragraph])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculate Relevance Scores with TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We import TfidfVectorizer from sklearn.feature_extraction.text to calculate TF-IDF scores.\n",
    "2. After scraping paragraphs from all URLs, we calculate TF-IDF scores for each paragraph using TfidfVectorizer.\n",
    "3. Finally, we write the paragraphs along with their TF-IDF scores to a CSV file. Each row contains a paragraph and its corresponding TF-IDF score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# List of website URLs\n",
    "urls = [\n",
    "    \"https://www.aljazeera.net/encyclopedia/2016/6/6/%D8%A7%D9%84%D8%B0%D9%83%D8%A7%D8%A1-%D8%A7%D9%84%D8%A7%D8%B5%D8%B7%D9%86%D8%A7%D8%B9%D9%8A-%D8%AD%D9%8A%D9%86%D9%85%D8%A7-%D8%AA%D9%81%D9%83%D8%B1-%D8%A7%D9%84%D8%A2%D9%84%D8%A9\",\n",
    "    \"https://www.oracle.com/ae-ar/artificial-intelligence/what-is-ai/\",\n",
    "    \"https://aws.amazon.com/ar/what-is/artificial-intelligence/\"\n",
    "]\n",
    "\n",
    "# Function to scrape paragraphs from a URL\n",
    "def scrape_paragraphs_from_url(url):\n",
    "    try:\n",
    "        # Send HTTP request\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for invalid responses\n",
    "\n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract paragraphs\n",
    "        paragraphs = [p.get_text().strip() for p in soup.find_all(\"p\") if p.get_text().strip()]\n",
    "\n",
    "        return paragraphs\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main function to scrape paragraphs from all URLs\n",
    "def scrape_paragraphs_from_all_urls(urls):\n",
    "    all_paragraphs = []\n",
    "    for url in urls:\n",
    "        paragraphs = scrape_paragraphs_from_url(url)\n",
    "        if paragraphs:\n",
    "            all_paragraphs.extend(paragraphs)\n",
    "    return all_paragraphs\n",
    "\n",
    "# Call the main function to scrape paragraphs from all URLs\n",
    "all_paragraphs = scrape_paragraphs_from_all_urls(urls)\n",
    "\n",
    "# Calculate TF-IDF scores\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(all_paragraphs)\n",
    "\n",
    "# Write paragraphs with TF-IDF scores to a CSV file\n",
    "with open(\"paragraphs_with_scores.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Paragraph\", \"TF-IDF Score\"])\n",
    "    for i, paragraph in enumerate(all_paragraphs):\n",
    "        if paragraph:\n",
    "            tfidf_score = tfidf_matrix[i].sum()\n",
    "            writer.writerow([paragraph, tfidf_score])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 : Preprocessing NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1: ذكاء اصطناع تعبير طلق قدر تبدي آلة برامج حاكى قدر ذهن بشر تعلم استنتاج فعل أوضاع برمج آلة أن اسم حقل أكاديمي معني كيفية صنع حواسيب برامج قادر اتخاذ سلوك ذكي\n",
      "Sample 2: سم عصر حديث بارز طور إنس خدم إنجاز مهمة دقة كبير سرع عال موثوق رصين\n",
      "Sample 3: رافق إنجاز تحدي اقتصاد سب جو هائل توزيع ثروة دول متقدم نامة بين أفراد نفس بلد واحد صار تهديد كبير عدد وظائف\n",
      "Sample 4: عاد طرح مصطلح ذكاء اصطناع عالم حاسوب الأميركي جون مكارثي رقمرقم صاغ عام رقم عام شهد انعقاد مؤتمر علم كلية دارتموث الأميركية إشارة أبحاث جار آنذاك حول إمكان تصميم آلة ذكي قادر تقليد محاكاة عمل بشر\n",
      "Sample 5: تم إعلان مؤتمر حمل جمع تبرع مال دعم أبحاث وصول اختراع أشبه عقل بشر يم آلة عمل مفرد حاج إنس\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import qalsadi.lemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Discretization\n",
    "    discretized_text = re.sub(r'\\d+', '<رقم>', text)\n",
    "\n",
    "    # Text Cleaning\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', discretized_text)\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(cleaned_text)\n",
    "\n",
    "    # Stop Words Removal\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatization using qalsadi library\n",
    "    lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "    lemmas = lemmer.lemmatize_text(' '.join(filtered_tokens))\n",
    "\n",
    "    return ' '.join(lemmas)  # Return as a single string\n",
    "\n",
    "# Read data from the CSV file\n",
    "data_from_csv = []\n",
    "with open(\"paragraphs_with_scores.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        data_from_csv.append(row)  \n",
    "\n",
    "# Preprocess the data\n",
    "preprocessed_data = [preprocess_text(paragraph[0]) for paragraph in data_from_csv[1:]]\n",
    "\n",
    "# Adding preprocessed data to existing data\n",
    "for i, paragraph in enumerate(preprocessed_data):\n",
    "    data_from_csv[i + 1].append(paragraph)\n",
    "\n",
    "# Writting combined data to a new CSV file\n",
    "with open(\"paragraphs_with_scores.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(data_from_csv)\n",
    "\n",
    "# Printing a sample of the preprocessed data to verify\n",
    "for i, lemmas in enumerate(preprocessed_data[:5]):\n",
    "    print(f\"Sample {i+1}: {lemmas}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.8514540358342435, 4.164563147722379, 5.010368571788386, 6.258658968009476, 5.051413539726722, 5.439024235762571, 5.449427317191826, 5.888641606110186, 5.655707819239928, 4.8791732950371145, 6.516580708318667, 4.546160802324115, 6.4317610186423115, 5.526665424433725, 4.330748326261495, 5.068805909846542, 5.259708281951149, 5.171544225953361, 5.658538210611621, 5.797123116403549, 4.612830843280806, 5.61885802533116, 5.47879701554365, 4.545090547601191, 5.330211732393721, 4.900260052401799, 4.160820389123455, 6.189793118159144, 5.358373765410045, 5.388021054506444, 5.215969435930708, 4.708777830647453, 6.252724519525624, 5.2570521255412554, 5.6994557605197995, 6.0256879998796835, 6.257619160661595, 5.209969063281068, 6.811319023445039, 4.941736432417115, 5.098650412960419, 5.779025500126466, 6.309086769561074, 5.707415436257148, 1.888402711259873, 2.917805048541531, 1.4513855654334076, 6.190674906791254, 6.098700841461703, 4.806704334136747, 5.436218224019627, 6.839042310212961, 6.68553162214185, 7.553815370142909, 6.827705265916721, 3.848088678677802, 3.3143475617347717, 2.9344096390086127, 4.274900842587863, 4.562036772602281, 5.1621659170597605, 4.9535859089823315, 5.295624410595235, 7.593240150455065, 3.1609083089584686, 5.899921302057455, 4.204626236118416, 5.405271628736299, 4.380112276799269, 4.909389615677168, 5.297444913047892, 6.112955748544103, 4.842605234176698, 6.5924541370113445, 6.137260544771924, 3.1434212938184403, 6.180946410721406, 5.159136782789052, 4.495956139197318, 9.21439588517766, 5.2137524289209, 7.672060712908725, 6.363000084620379, 8.035310101324558, 3.0164911215285826, 6.374444626117785, 6.792989293816394, 5.641940902662491, 6.404384956701225, 4.4145492697826025, 6.3776387747275445, 7.1722269713065705, 6.137340611675411, 6.406890467179646, 2.427352622823327, 5.684863559566981, 6.990894508942429, 5.515693985991787, 6.853791833426202, 4.939579508761598, 6.299284163566334, 6.690186545407861, 4.614734261206515, 1.9919649355518976, 1.9792114228342514, 5.66452172534456, 2.222614623575866, 6.615438148974121, 1.987149724420629, 7.779703821959135, 2.02392610023398, 5.851077007523483, 2.372540779929507, 4.586166494128554, 7.428799315147385, 1.9918063455123225, 6.543401648205433, 4.702660002286511, 2.7341215031364303, 5.258281669323679, 5.117896426228628, 4.517631242719661, 5.511577731219723, 4.168861899902012, 6.107024255212496, 6.446848658016441, 5.807394247620608, 5.911353765100759, 4.870290797075429, 3.0708788777274916, 2.6754371124747847, 2.5497020352350983]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "# we have our preprocessed_text and labels (target) ready\n",
    "text_data = []\n",
    "labels = []\n",
    "with open(\"paragraphs_with_scores.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        text = row[\"text\"]\n",
    "        label = float(row[\"TF-IDF Score\"])  # TF-IDF score as label\n",
    "        text_data.append(text)\n",
    "        labels.append(label)\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Loading and preprocessing the data\n",
    "text_data = []\n",
    "scores = []\n",
    "\n",
    "with open(\"paragraphs_with_scores.csv\", mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        text = row[\"text\"]\n",
    "        score = float(row[\"TF-IDF Score\"])\n",
    "        text_data.append(text)\n",
    "        scores.append(score)\n",
    "\n",
    "# Tokenization and padding\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100)  # Maxlen is an example, adjust based on our data\n",
    "\n",
    "# Assuming 'label' is the column containing the labels\n",
    "X = padded_sequences\n",
    "y = labels \n",
    "\n",
    "# Convert to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 172ms/step - loss: 25.7185 - mae: 4.7835 - val_loss: 11.2923 - val_mae: 3.0560\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 7.9849 - mae: 2.5379 - val_loss: 3.6120 - val_mae: 1.6474\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 2.6770 - mae: 1.3860 - val_loss: 2.2183 - val_mae: 1.1984\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 2.1553 - mae: 1.0705 - val_loss: 2.4566 - val_mae: 1.1850\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 2.5330 - mae: 1.1956 - val_loss: 2.3997 - val_mae: 1.1815\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 2.2311 - mae: 1.0976 - val_loss: 2.2060 - val_mae: 1.1904\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1.7050 - mae: 0.9454 - val_loss: 2.3674 - val_mae: 1.2668\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 2.3727 - mae: 1.2080 - val_loss: 2.6115 - val_mae: 1.3475\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 1.9318 - mae: 1.1303 - val_loss: 2.3763 - val_mae: 1.2723\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.9369 - mae: 1.0722 - val_loss: 2.1979 - val_mae: 1.2011\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 2.1979 - mae: 1.2011\n",
      "RNN Model Evaluation - Loss: 2.1979167461395264, MAE: 1.2011380195617676\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "def create_rnn_model(vocab_size):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=128),\n",
    "        SimpleRNN(128, return_sequences=False),\n",
    "        Dense(1)  # For regression, no activation function here\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "rnn_model = create_rnn_model(vocab_size)\n",
    "rnn_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "rnn_history = rnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "rnn_model.save(\"rnn_model.keras\")\n",
    "\n",
    "# Evaluate the model\n",
    "rnn_eval = rnn_model.evaluate(X_val, y_val)\n",
    "print(f\"RNN Model Evaluation - Loss: {rnn_eval[0]}, MAE: {rnn_eval[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Manual Hyperparameter Tuning for RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with 64 units, learning rate 0.001, batch size 16\n",
      "Validation MAE: 1.4553802013397217\n",
      "Training model with 64 units, learning rate 0.001, batch size 32\n",
      "Validation MAE: 1.213857889175415\n",
      "Training model with 64 units, learning rate 0.001, batch size 64\n",
      "Validation MAE: 1.1840791702270508\n",
      "Training model with 64 units, learning rate 0.0001, batch size 16\n",
      "Validation MAE: 1.550737977027893\n",
      "Training model with 64 units, learning rate 0.0001, batch size 32\n",
      "Validation MAE: 3.0280940532684326\n",
      "Training model with 64 units, learning rate 0.0001, batch size 64\n",
      "Validation MAE: 4.217204570770264\n",
      "Training model with 128 units, learning rate 0.001, batch size 16\n",
      "Validation MAE: 1.2640485763549805\n",
      "Training model with 128 units, learning rate 0.001, batch size 32\n",
      "Validation MAE: 1.3441420793533325\n",
      "Training model with 128 units, learning rate 0.001, batch size 64\n",
      "Validation MAE: 1.2044482231140137\n",
      "Training model with 128 units, learning rate 0.0001, batch size 16\n",
      "Validation MAE: 1.7377886772155762\n",
      "Training model with 128 units, learning rate 0.0001, batch size 32\n",
      "Validation MAE: 1.4850858449935913\n",
      "Training model with 128 units, learning rate 0.0001, batch size 64\n",
      "Validation MAE: 2.94128155708313\n",
      "Training model with 256 units, learning rate 0.001, batch size 16\n",
      "Validation MAE: 1.3519362211227417\n",
      "Training model with 256 units, learning rate 0.001, batch size 32\n",
      "Validation MAE: 1.1974592208862305\n",
      "Training model with 256 units, learning rate 0.001, batch size 64\n",
      "Validation MAE: 1.1768113374710083\n",
      "Training model with 256 units, learning rate 0.0001, batch size 16\n",
      "Validation MAE: 1.1996893882751465\n",
      "Training model with 256 units, learning rate 0.0001, batch size 32\n",
      "Validation MAE: 1.2190978527069092\n",
      "Training model with 256 units, learning rate 0.0001, batch size 64\n",
      "Validation MAE: 1.193507194519043\n",
      "Best Validation MAE: 1.1768113374710083\n",
      "Best Hyperparameters: {'rnn_units': 256, 'learning_rate': 0.001, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_rnn_model(vocab_size, rnn_units):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=128),\n",
    "        SimpleRNN(rnn_units, return_sequences=False),\n",
    "        Dense(1)  # For regression\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define a list of hyperparameters to test\n",
    "rnn_units_list = [64, 128, 256]\n",
    "learning_rates = [0.001, 0.0001]\n",
    "batch_sizes = [16, 32, 64]\n",
    "epochs = 20  \n",
    "\n",
    "best_model = None\n",
    "best_val_mae = float('inf')\n",
    "best_hyperparams = {}\n",
    "\n",
    "for rnn_units in rnn_units_list:\n",
    "    for lr in learning_rates:\n",
    "        for batch_size in batch_sizes:\n",
    "            print(f\"Training model with {rnn_units} units, learning rate {lr}, batch size {batch_size}\")\n",
    "            \n",
    "            # Create and compile the model\n",
    "            rnn_model = create_rnn_model(vocab_size, rnn_units)\n",
    "            optimizer = Adam(learning_rate=lr)\n",
    "            rnn_model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "            \n",
    "            # Train the model\n",
    "            rnn_history = rnn_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), verbose=0)\n",
    "            \n",
    "            # Evaluate the model\n",
    "            val_loss, val_mae = rnn_model.evaluate(X_val, y_val, verbose=0)\n",
    "            print(f\"Validation MAE: {val_mae}\")\n",
    "            \n",
    "            # Save the best model\n",
    "            if val_mae < best_val_mae:\n",
    "                best_val_mae = val_mae\n",
    "                best_model = rnn_model\n",
    "                best_hyperparams = {'rnn_units': rnn_units, 'learning_rate': lr, 'batch_size': batch_size}\n",
    "                best_model.save(\"best_rnn_model.keras\")\n",
    "\n",
    "print(f\"Best Validation MAE: {best_val_mae}\")\n",
    "print(f\"Best Hyperparameters: {best_hyperparams}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Number of RNN units: Experiment with different numbers of units in the RNN layer.\n",
    "* Learning rate: Adjust the learning rate of the optimizer.\n",
    "* Batch size: Test different batch sizes.\n",
    "* Number of epochs: Increase or decrease the number of epochs to see the effect on performance.\n",
    "\n",
    "° Explanation\n",
    "*   rnn_units_list: A list of different numbers of units to try in the RNN layer.\n",
    "*   learning_rates: A list of different learning rates to test.\n",
    "*   batch_sizes: A list of different batch sizes to test.\n",
    "*   epochs: Set to 20 for more thorough training.\n",
    "*   The code loops through all combinations of these hyperparameters, trains a model for each combination, and evaluates its performance on the validation set. \n",
    "*   The model with the best validation MAE (Mean Absolute Error) is saved as the best model, and its hyperparameters are recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bidirectional RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 252ms/step - loss: 26.1260 - mae: 4.8838 - val_loss: 12.9269 - val_mae: 3.2641\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 9.6302 - mae: 2.7285 - val_loss: 4.7760 - val_mae: 1.8417\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 2.3557 - mae: 1.2105 - val_loss: 2.6533 - val_mae: 1.3740\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 2.2855 - mae: 1.2025 - val_loss: 3.0673 - val_mae: 1.4337\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 1.7490 - mae: 0.9725 - val_loss: 3.1874 - val_mae: 1.5598\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 2.1545 - mae: 1.2054 - val_loss: 1.8090 - val_mae: 1.0866\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.9904 - mae: 0.8049 - val_loss: 2.9480 - val_mae: 1.4662\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.9756 - mae: 0.7928 - val_loss: 1.4236 - val_mae: 0.9592\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - loss: 0.6716 - mae: 0.6054 - val_loss: 1.1856 - val_mae: 0.8214\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - loss: 0.4833 - mae: 0.4918 - val_loss: 1.6084 - val_mae: 1.1167\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.6084 - mae: 1.1167\n",
      "Bidirectional RNN Model Evaluation - Loss: 1.608376383781433, MAE: 1.1167089939117432\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "\n",
    "def create_birnn_model(vocab_size):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=128),\n",
    "        Bidirectional(SimpleRNN(128, return_sequences=False)),\n",
    "        Dense(1)  # For regression\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "birnn_model = create_birnn_model(vocab_size)\n",
    "birnn_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "birnn_history = birnn_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "birnn_model.save(\"birnn_model.keras\")\n",
    "\n",
    "# Evaluate the model\n",
    "birnn_eval = birnn_model.evaluate(X_val, y_val)\n",
    "print(f\"Bidirectional RNN Model Evaluation - Loss: {birnn_eval[0]}, MAE: {birnn_eval[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. GRU Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 412ms/step - loss: 28.4219 - mae: 5.1368 - val_loss: 28.9050 - val_mae: 5.1631\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 292ms/step - loss: 27.2981 - mae: 5.0026 - val_loss: 26.6421 - val_mae: 4.9344\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - loss: 23.9059 - mae: 4.6668 - val_loss: 21.2007 - val_mae: 4.3072\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 136ms/step - loss: 15.3753 - mae: 3.6368 - val_loss: 5.6694 - val_mae: 1.7982\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step - loss: 5.9664 - mae: 1.7358 - val_loss: 4.6723 - val_mae: 1.8898\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - loss: 2.8121 - mae: 1.4490 - val_loss: 2.9037 - val_mae: 1.4174\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 164ms/step - loss: 1.0379 - mae: 0.8007 - val_loss: 1.0409 - val_mae: 0.7920\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 131ms/step - loss: 1.4921 - mae: 1.0815 - val_loss: 1.2227 - val_mae: 0.8730\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 136ms/step - loss: 0.6359 - mae: 0.6114 - val_loss: 2.3893 - val_mae: 1.3133\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 139ms/step - loss: 0.7130 - mae: 0.6709 - val_loss: 1.7526 - val_mae: 1.0820\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 1.7526 - mae: 1.0820\n",
      "GRU Model Evaluation - Loss: 1.7525969743728638, MAE: 1.0819824934005737\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "def create_gru_model(vocab_size):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=128),\n",
    "        GRU(128, return_sequences=False),\n",
    "        Dense(1)  # For regression\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "gru_model = create_gru_model(vocab_size)\n",
    "gru_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "gru_history = gru_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "gru_model.save(\"gru_model.keras\")\n",
    "\n",
    "# Evaluate the model\n",
    "gru_eval = gru_model.evaluate(X_val, y_val)\n",
    "print(f\"GRU Model Evaluation - Loss: {gru_eval[0]}, MAE: {gru_eval[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 278ms/step - loss: 27.9732 - mae: 5.0711 - val_loss: 27.2851 - val_mae: 4.9542\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - loss: 22.7782 - mae: 4.4448 - val_loss: 4.7139 - val_mae: 1.8912\n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 187ms/step - loss: 2.8704 - mae: 1.3681 - val_loss: 3.3043 - val_mae: 1.3281\n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 183ms/step - loss: 2.9256 - mae: 1.2795 - val_loss: 2.3226 - val_mae: 1.1836\n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 125ms/step - loss: 2.1725 - mae: 1.0878 - val_loss: 2.5138 - val_mae: 1.3096\n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - loss: 2.3391 - mae: 1.2498 - val_loss: 2.7387 - val_mae: 1.3886\n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 126ms/step - loss: 2.4988 - mae: 1.2933 - val_loss: 2.4202 - val_mae: 1.2825\n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step - loss: 2.0209 - mae: 1.1144 - val_loss: 2.1780 - val_mae: 1.1926\n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 188ms/step - loss: 2.0711 - mae: 1.0680 - val_loss: 2.1567 - val_mae: 1.1708\n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 145ms/step - loss: 1.9772 - mae: 1.0285 - val_loss: 2.1099 - val_mae: 1.1631\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 2.1099 - mae: 1.1631\n",
      "LSTM Model Evaluation - Loss: 2.109853744506836, MAE: 1.1630810499191284\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "def create_lstm_model(vocab_size):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=128),\n",
    "        LSTM(128, return_sequences=False),\n",
    "        Dense(1)  # For regression\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "lstm_model = create_lstm_model(vocab_size)\n",
    "lstm_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
    "# Best Hyperparameters: {'model_type': 'lstm', 'rnn_units': 64, 'learning_rate': 0.001, 'batch_size': 16}\n",
    "# Train the model\n",
    "lstm_history = lstm_model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "lstm_model.save(\"lstm_model.keras\")\n",
    "\n",
    "# Evaluate the model\n",
    "lstm_eval = lstm_model.evaluate(X_val, y_val)\n",
    "print(f\"LSTM Model Evaluation - Loss: {lstm_eval[0]}, MAE: {lstm_eval[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ° Hyperparameter Tuning for Bidirectional RNN, GRU, and LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training bidirectional_rnn model with 64 units, learning rate 0.001, batch size 16\n",
      "Validation MAE: 0.970288872718811\n",
      "Training bidirectional_rnn model with 64 units, learning rate 0.001, batch size 32\n",
      "Validation MAE: 1.5508111715316772\n",
      "Training bidirectional_rnn model with 64 units, learning rate 0.001, batch size 64\n",
      "Validation MAE: 1.2122737169265747\n",
      "Training bidirectional_rnn model with 64 units, learning rate 0.0001, batch size 16\n",
      "Validation MAE: 1.2876192331314087\n",
      "Training bidirectional_rnn model with 64 units, learning rate 0.0001, batch size 32\n",
      "Validation MAE: 1.6095991134643555\n",
      "Training bidirectional_rnn model with 64 units, learning rate 0.0001, batch size 64\n",
      "Validation MAE: 3.4364511966705322\n",
      "Training bidirectional_rnn model with 128 units, learning rate 0.001, batch size 16\n",
      "Validation MAE: 1.3526067733764648\n",
      "Training bidirectional_rnn model with 128 units, learning rate 0.001, batch size 32\n",
      "Validation MAE: 1.1391572952270508\n",
      "Training bidirectional_rnn model with 128 units, learning rate 0.001, batch size 64\n",
      "Validation MAE: 1.3927103281021118\n",
      "Training bidirectional_rnn model with 128 units, learning rate 0.0001, batch size 16\n",
      "Validation MAE: 1.209425926208496\n",
      "Training bidirectional_rnn model with 128 units, learning rate 0.0001, batch size 32\n",
      "Validation MAE: 1.1901429891586304\n",
      "Training bidirectional_rnn model with 128 units, learning rate 0.0001, batch size 64\n",
      "Validation MAE: 1.228162169456482\n",
      "Training bidirectional_rnn model with 256 units, learning rate 0.001, batch size 16\n",
      "Validation MAE: 1.204682469367981\n",
      "Training bidirectional_rnn model with 256 units, learning rate 0.001, batch size 32\n",
      "Validation MAE: 1.2765283584594727\n",
      "Training bidirectional_rnn model with 256 units, learning rate 0.001, batch size 64\n",
      "Validation MAE: 1.1797327995300293\n",
      "Training bidirectional_rnn model with 256 units, learning rate 0.0001, batch size 16\n",
      "Validation MAE: 1.298958659172058\n",
      "Training bidirectional_rnn model with 256 units, learning rate 0.0001, batch size 32\n",
      "Validation MAE: 1.2011430263519287\n",
      "Training bidirectional_rnn model with 256 units, learning rate 0.0001, batch size 64\n",
      "Validation MAE: 1.1772325038909912\n",
      "Training gru model with 64 units, learning rate 0.001, batch size 16\n",
      "Validation MAE: 0.5889485478401184\n",
      "Training gru model with 64 units, learning rate 0.001, batch size 32\n",
      "Validation MAE: 0.8049687147140503\n",
      "Training gru model with 64 units, learning rate 0.001, batch size 64\n",
      "Validation MAE: 1.7195014953613281\n",
      "Training gru model with 64 units, learning rate 0.0001, batch size 16\n",
      "Validation MAE: 2.4886534214019775\n",
      "Training gru model with 64 units, learning rate 0.0001, batch size 32\n",
      "Validation MAE: 5.016287326812744\n",
      "Training gru model with 64 units, learning rate 0.0001, batch size 64\n",
      "Validation MAE: 5.157407760620117\n",
      "Training gru model with 128 units, learning rate 0.001, batch size 16\n",
      "Validation MAE: 0.5546616315841675\n",
      "Training gru model with 128 units, learning rate 0.001, batch size 32\n",
      "Validation MAE: 0.7071407437324524\n",
      "Training gru model with 128 units, learning rate 0.001, batch size 64\n",
      "Validation MAE: 1.4132975339889526\n",
      "Training gru model with 128 units, learning rate 0.0001, batch size 16\n",
      "Validation MAE: 0.9430630207061768\n",
      "Training gru model with 128 units, learning rate 0.0001, batch size 32\n",
      "Validation MAE: 4.179092884063721\n",
      "Training gru model with 128 units, learning rate 0.0001, batch size 64\n",
      "Validation MAE: 5.130791664123535\n",
      "Training gru model with 256 units, learning rate 0.001, batch size 16\n",
      "Validation MAE: 0.858379602432251\n",
      "Training gru model with 256 units, learning rate 0.001, batch size 32\n",
      "Validation MAE: 1.0375723838806152\n",
      "Training gru model with 256 units, learning rate 0.001, batch size 64\n",
      "Validation MAE: 0.72457355260849\n",
      "Training gru model with 256 units, learning rate 0.0001, batch size 16\n",
      "Validation MAE: 0.7358120679855347\n",
      "Training gru model with 256 units, learning rate 0.0001, batch size 32\n",
      "Validation MAE: 1.11211097240448\n",
      "Training gru model with 256 units, learning rate 0.0001, batch size 64\n",
      "Validation MAE: 4.870090007781982\n",
      "Training lstm model with 64 units, learning rate 0.001, batch size 16\n",
      "Validation MAE: 0.29131290316581726\n",
      "Training lstm model with 64 units, learning rate 0.001, batch size 32\n",
      "Validation MAE: 0.45119890570640564\n",
      "Training lstm model with 64 units, learning rate 0.001, batch size 64\n",
      "Validation MAE: 0.47479093074798584\n",
      "Training lstm model with 64 units, learning rate 0.0001, batch size 16\n",
      "Validation MAE: 1.2260762453079224\n",
      "Training lstm model with 64 units, learning rate 0.0001, batch size 32\n",
      "Validation MAE: 3.389925241470337\n",
      "Training lstm model with 64 units, learning rate 0.0001, batch size 64\n",
      "Validation MAE: 4.9941277503967285\n",
      "Training lstm model with 128 units, learning rate 0.001, batch size 16\n",
      "Validation MAE: 0.3851901888847351\n",
      "Training lstm model with 128 units, learning rate 0.001, batch size 32\n",
      "Validation MAE: 0.3579166531562805\n",
      "Training lstm model with 128 units, learning rate 0.001, batch size 64\n",
      "Validation MAE: 0.4991214871406555\n",
      "Training lstm model with 128 units, learning rate 0.0001, batch size 16\n",
      "Validation MAE: 1.108889102935791\n",
      "Training lstm model with 128 units, learning rate 0.0001, batch size 32\n",
      "Validation MAE: 1.1988471746444702\n",
      "Training lstm model with 128 units, learning rate 0.0001, batch size 64\n",
      "Validation MAE: 4.539580345153809\n",
      "Training lstm model with 256 units, learning rate 0.001, batch size 16\n",
      "Validation MAE: 0.2978467643260956\n",
      "Training lstm model with 256 units, learning rate 0.001, batch size 32\n",
      "Validation MAE: 0.4179290533065796\n",
      "Training lstm model with 256 units, learning rate 0.001, batch size 64\n",
      "Validation MAE: 1.1571722030639648\n",
      "Training lstm model with 256 units, learning rate 0.0001, batch size 16\n",
      "Validation MAE: 0.9133094549179077\n",
      "Training lstm model with 256 units, learning rate 0.0001, batch size 32\n",
      "Validation MAE: 1.1894948482513428\n",
      "Training lstm model with 256 units, learning rate 0.0001, batch size 64\n",
      "Validation MAE: 1.1975735425949097\n",
      "Best Validation MAE: 0.29131290316581726\n",
      "Best Hyperparameters: {'model_type': 'lstm', 'rnn_units': 64, 'learning_rate': 0.001, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, LSTM, GRU\n",
    "\n",
    "def create_bidirectional_rnn_model(vocab_size, rnn_units):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=128),\n",
    "        Bidirectional(SimpleRNN(rnn_units, return_sequences=False)),\n",
    "        Dense(1)  \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_gru_model(vocab_size, rnn_units):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=128),\n",
    "        GRU(rnn_units, return_sequences=False),\n",
    "        Dense(1)  \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_lstm_model(vocab_size, rnn_units):\n",
    "    model = Sequential([\n",
    "        Embedding(input_dim=vocab_size, output_dim=128),\n",
    "        LSTM(rnn_units, return_sequences=False),\n",
    "        Dense(1)  \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Defining a list of hyperparameters to test\n",
    "rnn_units_list = [64, 128, 256]\n",
    "learning_rates = [0.001, 0.0001]\n",
    "batch_sizes = [16, 32, 64]\n",
    "epochs = 20  # we can increase the number of epochs for better tuning\n",
    "\n",
    "best_model = None\n",
    "best_val_mae = float('inf')\n",
    "best_hyperparams = {}\n",
    "\n",
    "for model_type in ['bidirectional_rnn', 'gru', 'lstm']:\n",
    "    for rnn_units in rnn_units_list:\n",
    "        for lr in learning_rates:\n",
    "            for batch_size in batch_sizes:\n",
    "                print(f\"Training {model_type} model with {rnn_units} units, learning rate {lr}, batch size {batch_size}\")\n",
    "                \n",
    "                if model_type == 'bidirectional_rnn':\n",
    "                    model = create_bidirectional_rnn_model(vocab_size, rnn_units)\n",
    "                elif model_type == 'gru':\n",
    "                    model = create_gru_model(vocab_size, rnn_units)\n",
    "                elif model_type == 'lstm':\n",
    "                    model = create_lstm_model(vocab_size, rnn_units)\n",
    "                    \n",
    "                optimizer = Adam(learning_rate=lr)\n",
    "                model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "                \n",
    "                history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_val, y_val), verbose=0)\n",
    "                \n",
    "                val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)\n",
    "                print(f\"Validation MAE: {val_mae}\")\n",
    "                \n",
    "                if val_mae < best_val_mae:\n",
    "                    best_val_mae = val_mae\n",
    "                    best_model = model\n",
    "                    best_hyperparams = {'model_type': model_type, 'rnn_units': rnn_units, 'learning_rate': lr, 'batch_size': batch_size}\n",
    "                    best_model.save(f\"best_{model_type}_model.keras\")\n",
    "\n",
    "print(f\"Best Validation MAE: {best_val_mae}\")\n",
    "print(f\"Best Hyperparameters: {best_hyperparams}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided validation MAE scores and hyperparameters, we can compare the performance of the RNN, Bidirectional RNN, GRU, and LSTM models:\n",
    "\n",
    "1. **RNN Model:**\n",
    "   - Validation MAE: 1.1768\n",
    "   - Hyperparameters: {'rnn_units': 256, 'learning_rate': 0.001, 'batch_size': 64}\n",
    "\n",
    "2. **Bidirectional RNN Model:**\n",
    "   - Validation MAE: 0.9703\n",
    "   - Hyperparameters: {'rnn_units': 64, 'learning_rate': 0.001, 'batch_size': 16}\n",
    "\n",
    "3. **GRU Model:**\n",
    "   - Validation MAE: 0.5547\n",
    "   - Hyperparameters: {'rnn_units': 128, 'learning_rate': 0.001, 'batch_size': 16}\n",
    "\n",
    "4. **LSTM Model:**\n",
    "   - Validation MAE: 0.2913\n",
    "   - Hyperparameters: {'rnn_units': 64, 'learning_rate': 0.001, 'batch_size': 16}\n",
    "\n",
    "**Comparison:**\n",
    "- The LSTM model achieved the lowest validation MAE, indicating the best performance among the four models.\n",
    "- The Bidirectional RNN model performed better than the RNN model but not as well as the GRU and LSTM models.\n",
    "- The GRU model outperformed both the RNN and Bidirectional RNN models but had a slightly higher validation MAE compared to the LSTM model.\n",
    "- Overall, the LSTM model with 64 units, a learning rate of 0.001, and a batch size of 16 appears to be the best choice based on the provided validation results and hyperparameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
